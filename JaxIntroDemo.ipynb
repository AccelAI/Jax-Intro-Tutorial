{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JaxIntroDemo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOU6lBXO09U8nQMilODnEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AccelAI/Jax-Intro-Tutorial/blob/main/JaxIntroDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OTElDdmpNj1"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnEqaVJunujj"
      },
      "source": [
        "The first thing we need to do is make sure that we have all the packages that we need. If you are installing on your own machine, make sure you are running python 3 and then you can install anaconda for everything we need (and more) or each of the following individually. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGiHf5ktnACr"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install numpy\n",
        "!{sys.executable} -m pip install torch\n",
        "!{sys.executable} -m pip install jax\n",
        "!{sys.executable} -m pip install jaxlib\n",
        "!{sys.executable} -m pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucRyH97DyLH6"
      },
      "source": [
        "Next lets import our first jax modules and do some basic math to test out Jax vs NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY8mDvEvCGqk"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import grad, jit, vmap, random, lax\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1vAsV9oygkP"
      },
      "source": [
        "# Prevent GPU/TPU warning.\n",
        "import jax; jax.config.update('jax_platform_name', 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HOxLdW1pT84"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT3ipTQi228k"
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "x = random.normal(key, (10,))\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxN2MjIup0cP"
      },
      "source": [
        "### Numpy vs Jax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBHTy38X_8X_"
      },
      "source": [
        "size = 3000\n",
        "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
        "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3KDG_hdy0Ed"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPVNc6QprIG"
      },
      "source": [
        "### Basic Multiplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feZ3jl2SA2O2"
      },
      "source": [
        "a = jnp.array(3.)\n",
        "b = jnp.array([3., 2., 1.])\n",
        "c = jnp.array([5., 5., 5.])\n",
        "\n",
        "def dot_mul(a, b, c):\n",
        "  return (a * jnp.dot(b, c))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7498SYQQBa_j"
      },
      "source": [
        "dot_mul(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZj5wsk5pkjq"
      },
      "source": [
        "## Gradient\n",
        "\n",
        "using jax.grad with out function dot_mul we can get the gradient of our function return with respect to a parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3_wYgDIh8by"
      },
      "source": [
        "grad(dot_mul)(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhfyY41ttaFm"
      },
      "source": [
        "grad(dot_mul, argnums=[1])(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIX6AnLTtifj"
      },
      "source": [
        "grad(dot_mul, argnums=[2])(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naUocxutpnQ9"
      },
      "source": [
        "## VMap\n",
        "Dealing with batches of data, ie matrices "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XCbPNjRzCjh"
      },
      "source": [
        "a2 = jnp.array([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.], [4., 4., 4.]])\n",
        "a2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsJLNugG0QfA"
      },
      "source": [
        "Gradient is only designed for scalar output which does not support batched data while vmap is built for working with matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiUDZKyszV3f"
      },
      "source": [
        "grad(dot_mul)(a, a2, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULWJACn_zlu_"
      },
      "source": [
        "vmap(dot_mul, in_axes=(None, None, 0))(a, b, a2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5Njhnh00bE"
      },
      "source": [
        "If you are looking to get the gradients across a batch of data you can combine the two"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoE8Fb8O06Er"
      },
      "source": [
        "vmap(grad(dot_mul), in_axes=(None, None, 0))(a, b, a2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E10Aa-hw1Hhu"
      },
      "source": [
        "## Just in Time compilation (JIT)\n",
        "As python is an interpreted language it can be quite slow, especially when dealing with large data sets. To fix this we use JIT which is just like tf-function or autograph in tensorflow and typescript in pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s_yJJTc2Moz"
      },
      "source": [
        "jit(dot_mul)(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r40EQd6-2hYN"
      },
      "source": [
        "jax.jit(jax.vmap(jax.grad(dot_mul), in_axes=(None, None, 0)))(a, b, a2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGRQxLowCAsH"
      },
      "source": [
        "We can actually see what JIT is doing with jaxpr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbHCmfzUCYwm"
      },
      "source": [
        "jax.jit(dot_mul) # returns compiled version of our function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4J_Xde7CEgu"
      },
      "source": [
        "jax.make_jaxpr(jax.jit(dot_mul))(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlIKbuaDC47s"
      },
      "source": [
        "## PMAP (Parallel Map)\n",
        "Distributing computation across hardware (GPUs / TPUs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrpGK88zxW_e",
        "outputId": "e47d8e96-73ff-4dbf-f6f8-27de61399196"
      },
      "source": [
        "# get the latest JAX and jaxlib\n",
        "!pip install --upgrade -q jax jaxlib\n",
        "\n",
        "# Colab runtime set to TPU accel\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver_nightly'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# TPU driver as backend for JAX\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grpc://10.75.90.202:8470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEmEaFJPxnVD",
        "outputId": "b8d44d1a-cf24-428f-8a7d-d713fe61c0db"
      },
      "source": [
        "jax.local_devices()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<jaxlib.xla_extension.Device at 0x7f59e6300130>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8BP3jcJ2dBv"
      },
      "source": [
        "y = jax.pmap(lambda x: x ** 2)(jnp.arange(8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GczEmDK5wGJa"
      },
      "source": [
        "a = jnp.array([1., 2., 3., 4., 5., 6.,7., 8.])\n",
        "jax.pmap(dot_mul, in_axes=(0, None, None))(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}